{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2525fb2f",
   "metadata": {},
   "source": [
    "<div style=\"align: center; margin: 0; padding: 0; height: 250px;\">\n",
    "    <br>\n",
    "    <img src=\"https://www.nyc.gov/assets/tlc/images/content/hero/MRP-Closing-Week.jpg\" style=\"display:block; margin:auto; width:65%; height:100%;\">\n",
    "</div><br><br> \n",
    "\n",
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "<!--   https://xkcd.com/color/rgb/   -->\n",
    "  <p style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>TLC Trip Record Data</strong></p>  \n",
    "  \n",
    "  <p style=\"text-align:center; background-color:romance; color: Jaguar; border-radius:10px; font-family:monospace; \n",
    "            line-height:1.4; font-size:22px; font-weight:normal; text-transform: capitalize; padding: 5px;\"\n",
    "     >Machine Learning Module: PREFECT - Ride Duration Prediction<br>using Regression Analysis<br></p><br>\n",
    "    \n",
    "  <div style=\"align: center;\">\n",
    "  <table style=\"text-align: center; background-color: romance; color: Jaguar; border-radius: 10px; font-family: monospace;\n",
    "                  line-height:1.4; font-size: 21px; font-weight: normal; text-transform: capitalize; padding: 5px; \n",
    "                  margin: 0 auto;\">\n",
    "    <tr><td style=\"text-align: left; padding-left: 0px;\"\n",
    "            > PREFECT <span style=\"font-size: 16px;\">(framework for building, Deploying,<br>Scheduling, and Monitoring Data Pipelines and Workflows)</span></td></tr>\n",
    "    <tr><td style=\"text-align: left; padding-left: 0px;\"\n",
    "            > MLFLOW <span style=\"font-size: 16px;\">(Machine Learning Flow)</span></td></tr>\n",
    "    <tr><td style=\"text-align: left; padding-left: 0px;\"\n",
    "            > MLOps <span style=\"font-size: 16px;\">(CI/CD, Model Versioning, Monitoring, Automated Retraining,<br>Security, Scalability, Collaboration)</span></td></tr>\n",
    "  </table>\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "- https://docs.prefect.io/2.11.3/\n",
    "- https://github.com/discdiver/prefect-mlops-zoomcamp\n",
    "- https://mlflow.org/docs/0.7.0/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8b00d",
   "metadata": {},
   "source": [
    "**Dataset Info**\n",
    "\n",
    "\n",
    "**Context**\n",
    "\n",
    "Yellow and green taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP). The trip data was not created by the TLC, and TLC makes no representations as to the accuracy of these data.\n",
    "\n",
    "For-Hire Vehicle (“FHV”) trip records include fields capturing the dispatching base license number and the pick-up date, time, and taxi zone location ID (shape file below). These records are generated from the FHV Trip Record submissions made by bases. Note: The TLC publishes base trip record data as submitted by the bases, and we cannot guarantee or confirm their accuracy or completeness. Therefore, this may not represent the total amount of trips dispatched by all TLC-licensed bases. The TLC performs routine reviews of the records and takes enforcement actions when necessary to ensure, to the extent possible, complete and accurate information.\n",
    "\n",
    "\n",
    "**ATTENTION!**\n",
    "\n",
    "On 05/13/2022, we are making the following changes to trip record files:\n",
    "\n",
    "- All files will be stored in the PARQUET format. Please see the ‘Working With PARQUET Format’ under the Data Dictionaries and MetaData section.\n",
    "- Trip data will be published monthly (with two months delay) instead of bi-annually.\n",
    "- HVFHV files will now include 17 more columns (please see High Volume FHV Trips Dictionary for details). Additional columns will be added to the old files as well. The earliest date to include additional columns: February 2019.\n",
    "- Yellow trip data will now include 1 additional column (‘airport_fee’, please see Yellow Trips Dictionary for details). The additional column will be added to the old files as well. The earliest date to include the additional column: January 2011.\n",
    "\n",
    "\n",
    "**Download the Data**\n",
    "\n",
    "> `Dataset`: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "\n",
    "**Data Dictionaries and MetaData**\n",
    "\n",
    "> `Green Trips Data Dictionary`: https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf<br>\n",
    "> `Yellow Trips Data Dictionary`: https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\n",
    "\n",
    "**TASK**\n",
    "\n",
    "The goal of this homework is to familiarize users with workflow orchestration. \n",
    "\n",
    "Start with the orchestrate.py file in the 03-orchestration/3.4 folder\n",
    "of the course repo: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/03-orchestration/3.4/orchestrate.py<br>\n",
    "\n",
    "Questions: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/cohorts/2023/03-orchestration/homework.md\n",
    "\n",
    "- https://sagarthacker.com/posts/mlops/intro_workflow_orchestration.html\n",
    "- https://sagarthacker.com/posts/mlops/prefect-blocks.html\n",
    "- https://sagarthacker.com/posts/mlops/prefect-deployment.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78138c11",
   "metadata": {},
   "source": [
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "  <h1 style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;\n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>1. Import Libraries & Ingest Data</strong></h1>   \n",
    "</div>\n",
    "\n",
    "> ⚠️ Not Recommended conda `base` env, work on using a Python virtual environment manager such as `pipenv`, `conda`, or `virtualenv/venv`.\n",
    "\n",
    "- https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf\n",
    "\n",
    "```\n",
    "pip freeze > requirements.txt\n",
    "conda list -e > requirements.txt\n",
    "\n",
    "# new conda virtual environment\n",
    "conda create --name \"prefect-ops\" python=3.10 jupyter -y\n",
    "conda activate \"prefect-ops\"\n",
    "\n",
    "# install all package dependencies\n",
    "pip install -r requirements.txt\n",
    "conda install -c conda-forge --file=requirements.txt      # mostly not work\n",
    "conda install -c conda-forge pandas==2.0.2 -q -y\n",
    "\n",
    "# if The environment is inconsistent, try below\n",
    "conda update -n base -c defaults conda --force-reinstall\n",
    "conda install anaconda --force-reinstall\n",
    "\n",
    "```\n",
    "\n",
    "**You must use the `--no-deps` option in the pip install command in order to avoid bundling dependencies into your conda-package.**\n",
    "\n",
    "If you run pip install without the `--no-deps` option, pip will often install dependencies in your conda recipe and those dependencies will become part of your package. This wastes space in the package and `increases the risk of file overlap`, file clobbering, and broken packages.\n",
    "\n",
    "There might be cases where you want to install a package directly from a local directory or a specific location, without relying on the package indexes. In such situations, you can use the `--no-index` option to tell pip not to look for the package in any indexes.\n",
    "\n",
    "```\n",
    "- command1 & command2  # runs simultaneously\n",
    "- command1 ; command2  # runs sequentially\n",
    "- command1 && command2 # runs sequentially, runs command2 only if command1 succeeds\n",
    "- command1 || command2 # runs sequentially, runs command2 only if command1 fails\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74723240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt \n",
    "# To get started with MLflow you'll need to install the appropriate Python package.\n",
    "\n",
    "# for parquet file\n",
    "pyarrow\n",
    "fastparquet\n",
    "# orjson is a fast, correct JSON library\n",
    "orjson\n",
    "pandas\n",
    "seaborn\n",
    "\n",
    "# ML Model packages\n",
    "scikit-learn\n",
    "xgboost\n",
    "optuna\n",
    "hyperopt\n",
    "\n",
    "# MLOPS experiment-tracking packages\n",
    "mlflow\n",
    "wandb\n",
    "prefect\n",
    "prefect-email\n",
    "\n",
    "# Optionally\n",
    "# for py file parameter\n",
    "click\n",
    "# for code style\n",
    "black\n",
    "# MLOPS Cloud packages\n",
    "boto3\n",
    "prefect-aws\n",
    "\n",
    "# Optionally\n",
    "jupyter\n",
    "ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db08661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python  : 3.10.12 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 19:09:20) [MSC v.1916 64 bit (AMD64)]\n",
      "Platform: Windows Windows-10-10.0.22621-SP0\n",
      "Actv Env: lin-reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform, IPython.display\n",
    "\n",
    "# pip install --no-deps --no-cache-dir --force-reinstall --no-index\n",
    "!{sys.executable} -m pip install -Uq -r requirements.txt --no-cache-dir  \n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "# IPython.display.clear_output()\n",
    "print(\"Python  :\", sys.version)\n",
    "print(\"Platform:\", platform.system(), platform.platform())\n",
    "print(\"Actv Env:\", os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ffeeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import stats\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import click\n",
    "import pickle\n",
    "# import pathlib\n",
    "# import argparse\n",
    "# import requests\n",
    "# import urllib.request\n",
    "from glob import glob\n",
    "\n",
    "# from tqdm import tqdm           # console-based\n",
    "# from tqdm.notebook import tqdm  # jupyter-based\n",
    "from tqdm.auto import tqdm        # automatically selects\n",
    "# tqdm._instances.clear()\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# import wandb\n",
    "import mlflow\n",
    "import prefect\n",
    "from prefect import task, flow, Flow\n",
    "from prefect.tasks import task_input_hash\n",
    "from prefect.artifacts import create_markdown_artifact\n",
    "\n",
    "# memory management performs garbage collection \n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3975cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Get the current working directory\n",
    "# current_dir = os.getcwd()\n",
    "\n",
    "# Create a new directory for storing data\n",
    "os.makedirs('./pycode', exist_ok=True)\n",
    "# os.makedirs('./data', exist_ok=True)\n",
    "# os.makedirs('./output', exist_ok=True)\n",
    "# os.makedirs('./models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37b7a6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:             2.11.3\n",
      "API version:         0.8.4\n",
      "Python version:      3.10.12\n",
      "Git commit:          3a400865\n",
      "Built:               Thu, Aug 3, 2023 3:24 PM\n",
      "OS/Arch:             win32/AMD64\n",
      "Profile:             default\n",
      "Server type:         ephemeral\n",
      "Server:\n",
      "  Database:          sqlite\n",
      "  SQLite version:    3.41.2\n"
     ]
    }
   ],
   "source": [
    "!prefect version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb564b98",
   "metadata": {},
   "source": [
    "<div style=\"letter-spacing:normal; opacity:1.;\">\n",
    "  <h1 style=\"text-align:center; background-color: lightsalmon; color: Jaguar; border-radius:10px; font-family:monospace; border-radius:20px;\n",
    "            line-height:1.4; font-size:32px; font-weight:bold; text-transform: uppercase; padding: 9px;\">\n",
    "            <strong>2. Recognizing and Understanding Data</strong></h1>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5b120",
   "metadata": {},
   "source": [
    "### Ingest Data [wget](https://linuxways.net/centos/linux-wget-command-with-examples/) or [curl](https://daniel.haxx.se/blog/2020/09/10/store-the-curl-output-over-there/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2a3fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Green Taxi Trip Records\" Download the data for January, February and March 2022\n",
    "# !wget -q -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-01.parquet\n",
    "# !wget -q -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-02.parquet\n",
    "# !wget -q -N -P \"./data\" https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-03.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "044ce591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(f'./data/*.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41e352",
   "metadata": {},
   "source": [
    "## Q1. Human-readable name\n",
    "\n",
    "You’d like to give the first task, `read_data` a nicely formatted name.\n",
    "How can you specify a task name?\n",
    "\n",
    "> Hint: look in the docs at https://docs.prefect.io or \n",
    "> check out the doc string in a code editor.\n",
    "\n",
    "- `@task(retries=3, retry_delay_seconds=2, name=\"Read taxi data\")`\n",
    "- `@task(retries=3, retry_delay_seconds=2, task_name=\"Read taxi data\")`\n",
    "- `@task(retries=3, retry_delay_seconds=2, task-name=\"Read taxi data\")`\n",
    "- `@task(retries=3, retry_delay_seconds=2, task_name_function=lambda x: f\"Read taxi data\")`\n",
    "\n",
    "Answers:\n",
    "- https://docs.prefect.io/2.10.13/concepts/tasks/\n",
    "- @task(retries=3, retry_delay_seconds=2, name=\"Read taxi data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159982ef",
   "metadata": {},
   "source": [
    "## Q2. Cron\n",
    "\n",
    "Cron is a common scheduling specification for workflows. \n",
    "\n",
    "Using the flow in `orchestrate.py`, create a deployment.\n",
    "Schedule your deployment to run on the third day of every month at 9am UTC.\n",
    "What’s the cron schedule for that?\n",
    "\n",
    "- `0 9 3 * *`\n",
    "- `0 0 9 3 *`\n",
    "- `9 * 3 0 *`\n",
    "- `* * 9 3 0`\n",
    "\n",
    "components of the cron expression:\n",
    "\n",
    "- The first field, 0, represents the minute of the hour. In this case, it is set to 0, meaning the deployment will run at the start of the hour.\n",
    "- The second field, 9, represents the hour of the day. It is set to 9, indicating that the deployment will run at 9am.\n",
    "- The third field, 3, represents the day of the month. This field is set to 3, which means the deployment will run specifically on the third day of each month.\n",
    "- The fourth field, *, represents the month. It is set to *, indicating that the deployment will run every month.\n",
    "- The fifth field, *, represents the day of the week. It is also set to *, meaning that the deployment will run regardless of the day of the week.\n",
    "\n",
    "Therefore, the cron schedule for running the deployment on the third day of every month at 9am UTC is:\n",
    "- `0 9 3 * *`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8f5e35",
   "metadata": {},
   "source": [
    "## Q3. RMSE \n",
    "\n",
    "Download the January 2023 Green Taxi data and use it for your training data.\n",
    "Download the February 2023 Green Taxi data and use it for your validation data. \n",
    "\n",
    "Make sure you upload the data to GitHub so it is available for your deployment.\n",
    "\n",
    "Create a custom flow run of your deployment from the UI. Choose Custom\n",
    "Run for the flow and enter the file path as a string on the JSON tab under Parameters.\n",
    "\n",
    "Make sure you have a worker running and polling the correct work pool.\n",
    "\n",
    "View the results in the UI.\n",
    "\n",
    "What’s the final RMSE to five decimal places?\n",
    "\n",
    "- 6.67433\n",
    "- 5.19931\n",
    "- 8.89443\n",
    "- 9.12250\n",
    "\n",
    "\n",
    "mlflow-remote code:\n",
    "- https://reimbar.org/dev/mlflow-remote/\n",
    "- https://www.mlflow.org/docs/latest/tracking.html#how-runs-and-artifacts-are-recorded\n",
    "\n",
    "Run in Terminal: \n",
    "```\n",
    "# AFTER set_experiment (building mlflow.db), go to bash cd path to mlruns folder\n",
    "mlflow ui \\\n",
    "    --backend-store-uri sqlite:///./mlflow.db \\\n",
    "    --default-artifact-root ./artifacts \\\n",
    "    --host localhost --port 5000\n",
    "\n",
    "# if using port 5000 or use another port\n",
    "kill $(lsof -ti :5000)   # clear port 5000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4e6c0",
   "metadata": {},
   "source": [
    "## Welcome to Prefect\n",
    "\n",
    "- https://docs.prefect.io/2.11.3/\n",
    "\n",
    "### Prefect is a workflow orchestration tool empowering developers to build, observe, and react to data pipelines.\n",
    "\n",
    "### Prefect Cloud is a workflow orchestration platform. Prefect Cloud provides all the capabilities of the Prefect server and UI in a hosted environment, plus additional features such as automations, workspaces, and organizations.\n",
    "\n",
    "<div style=\"height: 600px; margin: 0; padding: 0;\">\n",
    "  <img src=\"https://docs.prefect.io/latest/img/concepts/flow-deployment-end-to-end.png\" alt=\"flow-deployment\" style=\"width: 100%; height: 100%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae5287",
   "metadata": {},
   "source": [
    "## Prefect Quickstart\n",
    "\n",
    "### Step 0: Clone your Github Repo, then Check Data\n",
    "```sh\n",
    "git clone \n",
    "git remote -v\n",
    "```\n",
    "\n",
    "\n",
    "### Step 1: Install Prefect\n",
    "```sh\n",
    "pip install -U prefect\n",
    "```\n",
    "\n",
    "\n",
    "### Step 2: Connect to Prefect's API\n",
    "- Sign up for a forever free Prefect Cloud account or, alternatively, host your own server with a subset of Prefect Cloud's features.\n",
    "- Initialize the Prefect server locally\n",
    "\n",
    "Open another terminal and activate your conda environment. Start the Prefect API server locally with\n",
    "Start and Open Source Prefect Server\n",
    "```sh\n",
    "prefect server start\n",
    "```\n",
    "- Or Login, Prefect CLI command to log into Prefect Cloud from your environment.\n",
    "```sh\n",
    "prefect cloud login\n",
    "```\n",
    "\n",
    "\n",
    "### Step 3: Create a flow\n",
    "#### Rules of Thumb\n",
    "- At a minimum, you need to define at least one flow function.\n",
    "- Your flows can be segmented by introducing task (@task) functions, which can be invoked from within these flows.\n",
    "- A task represents a discrete unit of Python code, whereas flows are more akin to parent functions accommodating a broad range of workflow logic.\n",
    "- Flows can be called inside of other flows (we call these subflows) but a task cannot be run inside of another task or from outside the context of a flow.\n",
    "\n",
    "```py\n",
    "# @flow\n",
    "\n",
    "# Add your original code here\n",
    "# ...\n",
    "```\n",
    "\n",
    "**In this task, the code will be modified and improved by adding an @flow decorator to the entrypoint function.**\n",
    "- First, modify the existing code by adding an @flow decorator to the code's entrypoint function. The @flow decorator helps in type checking and ensures more robust code.\n",
    "- Specify a name for the @flow decorator. For example, you can name it \"main\" if the entrypoint function is named \"main.\"\n",
    "- After adding the @flow decorator, make sure to save the modified code to a new directory. This ensures that you keep a copy of the enhanced code separately. if use `Github`, make sure **to push** the modified **code** and available **data**.\n",
    "\n",
    "\n",
    "### Step 4: Run your flow locally\n",
    "\n",
    "- Call any function that you've decorated with a @flow decorator to see a local instance of a flow run.\n",
    "```sh\n",
    "python my_flow.py\n",
    "```\n",
    "\n",
    "\n",
    "### Step 5: Deploy the flow\n",
    "\n",
    "#### **Always run prefect deploy commands from the root level of your repo!**\n",
    "Common Pitfalls:\n",
    "- When running prefect deploy or prefect init commands, double check that you are at the root of your repo, otherwise the worker may attempt to use an incorrect flow entrypoint during remote execution!\n",
    "- Ensure that you have pushed any changes to your flow script to your GitHub repo - at any given time, your worker will pull the code that exists there!\n",
    "```sh\n",
    "prefect deploy\n",
    "prefect deploy ls\n",
    "prefect deploy --all\n",
    "prefect deploy -n 'deployment_mlops_zoom'\n",
    "```\n",
    "\n",
    "It's recommended to save the configuration for the deployment.\n",
    "\n",
    "- Saving the configuration for your deployment will result in a prefect.yaml file populated with your first deployment. You can use this YAML file to edit and [define multiple deployments](https://docs.prefect.io/concepts/deployments-ux/) for this repo.\n",
    "\n",
    "\n",
    "### Step 6: Start a worker and run the deployed flow\n",
    "\n",
    "- Start a worker to manage local flow execution. Each worker polls its assigned work pool.\n",
    "\n",
    "In a new terminal, run:\n",
    "```sh\n",
    "prefect worker start --pool '<work-pool-name>'\n",
    "prefect worker start --pool 'pool_mlops_zoom'\n",
    "prefect worker start -p 'pool_mlops_zoom' -t process --limit 2\n",
    "```\n",
    "\n",
    "Now that your worker is running, you are ready to kick off deployed flow runs from the UI or by running:\n",
    "```sh\n",
    "prefect deployment ls\n",
    "prefect deployment run '<flow-name>/<deployment-name>'\n",
    "prefect deployment run 'Main Flow/deployment_mlops_zoom'\n",
    "```\n",
    "\n",
    "Check out this flow run's logs from the Flow Runs page in the UI or from the worker logs. Congrats on your first successfully deployed flow run! 🎉\n",
    "\n",
    "You've seen:\n",
    "- how to define your flows and tasks using decorators\n",
    "- how to deploy a flow\n",
    "- how to start a worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f5035",
   "metadata": {},
   "source": [
    "**NOTE:** In the below Libraries `NOT WORK` with `PREFECT DEPLOYMENT CODE`, So we rewrite Script below for `DEPLOYMENT`.\n",
    "\n",
    "    - import argparse\n",
    "    - import click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ce59af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pycode/orchestrate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/orchestrate.py\n",
    "\n",
    "# Source: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/03-orchestration/3.4/orchestrate.py\n",
    "\n",
    "import os\n",
    "import click\n",
    "import pickle\n",
    "\n",
    "import pathlib\n",
    "import argparse\n",
    "import requests\n",
    "import urllib.request\n",
    "from glob import glob\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "import mlflow\n",
    "import prefect\n",
    "from prefect import task, flow\n",
    "from prefect.tasks import task_input_hash\n",
    "from prefect.artifacts import create_markdown_artifact\n",
    "\n",
    "# from prefect_aws import S3Bucket\n",
    "# from prefect_email import EmailServerCredentials, email_send_message\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# Filter the specific warning message, MLflow autologging encountered a warning\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"setuptools\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Setuptools is replacing distutils.\")\n",
    "\n",
    "\n",
    "@task(name=\"Fetch Data\", cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1),\n",
    "      retries=3, log_prints=True, )\n",
    "def fetch_data(raw_data_path: str, year: int, month: int, color: str) -> None:\n",
    "    \"\"\"Fetches data from the NYC Taxi dataset and saves it locally\"\"\"\n",
    "    url      = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{color}_tripdata_{year}-{month:0>2}.parquet'    \n",
    "    filename = os.path.join(raw_data_path, f'{color}_tripdata_{year}-{month:0>2}.parquet')\n",
    "\n",
    "    # Create dest_path folder unless it already exists\n",
    "    os.makedirs(raw_data_path, exist_ok=True)\n",
    "    \n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    # os.system(f\"wget -q -N -P {raw_data_path} {url}\")\n",
    "    # urllib.request.urlretrieve(url, filename)\n",
    "    response = requests.get(url)\n",
    "    with open(filename, \"wb\") as f_out:\n",
    "        f_out.write(response.content)\n",
    "    return None\n",
    "\n",
    "\n",
    "@flow(name=\"Download Datas\", log_prints=True)\n",
    "def download_data(raw_data_path: str, years: list, months: list, colors: list) -> None:\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            for color in colors:\n",
    "                fetch_data(raw_data_path, year, month, color)\n",
    "    return None\n",
    "    \n",
    "    \n",
    "@task(name=\"Read Data\", retries=3, retry_delay_seconds=2, log_prints=None)\n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "    df.lpep_pickup_datetime  = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "\n",
    "    df[\"duration\"] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df[\"duration\"] = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical     = [\"PULocationID\", \"DOLocationID\"]\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "@task(name=\"Preprocess: Add Features\", log_prints=True)\n",
    "def preprocess(\n",
    "    df: pd.DataFrame,dv: DictVectorizer = None, fit_dv: bool = False\n",
    ") -> tuple(\n",
    "    [\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        np.ndarray,\n",
    "        sklearn.feature_extraction.DictVectorizer,\n",
    "    ]\n",
    "):\n",
    "    \"\"\"Add features to the model\"\"\"\n",
    "    df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']\n",
    "    categorical = [\"PU_DO\"]\n",
    "    numerical   = ['trip_distance']\n",
    "    dicts       = df[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    if fit_dv:\n",
    "        # return sparse matrix\n",
    "        dv = DictVectorizer()\n",
    "        X = dv.fit_transform(dicts)\n",
    "    else:\n",
    "        X = dv.transform(dicts)\n",
    "        \n",
    "    # Convert X the sparse matrix  to pandas DataFrame, but too slow\n",
    "    # X = pd.DataFrame(X.toarray(), columns=dv.get_feature_names_out())\n",
    "    # X = pd.DataFrame.sparse.from_spmatrix(X, columns=dv.get_feature_names_out())\n",
    "\n",
    "    try:\n",
    "        # Extract the target\n",
    "        target = 'duration'\n",
    "        y = df[target].values\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return (X, y), dv\n",
    "\n",
    "\n",
    "@task(name=\"Train Best Model\", log_prints=True)\n",
    "def train_best_model(\n",
    "    X_train  : scipy.sparse._csr.csr_matrix,\n",
    "    X_val    : scipy.sparse._csr.csr_matrix,\n",
    "    y_train  : np.ndarray,\n",
    "    y_val    : np.ndarray,\n",
    "    dv       : sklearn.feature_extraction.DictVectorizer,\n",
    "    raw_data_path: str,\n",
    "    dest_path: str,\n",
    ") -> None:\n",
    "    \"\"\"train a model with best hyperparams and write everything out\"\"\"        \n",
    "    # Load train and test Data\n",
    "    train = xgb.DMatrix(X_train, label=y_train)\n",
    "    valid = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    # MLflow settings\n",
    "    # Build or Connect Database Offline\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    # Connect Database Online\n",
    "    # mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "    \n",
    "    # Build or Connect mlflow experiment\n",
    "    EXPERIMENT_NAME = \"nyc-taxi-experiment\"\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "    # before your training code to enable automatic logging of sklearn metrics, params, and models\n",
    "    # mlflow.xgboost.autolog()\n",
    "    \n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Optional: Set some information about Model\n",
    "        mlflow.set_tag(\"developer\", \"muce\")\n",
    "        mlflow.set_tag(\"algorithm\", \"Machine Learning\")\n",
    "        mlflow.set_tag(\"train-data-path\", f'{raw_data_path}/green_tripdata_2023-01.parquet')\n",
    "        mlflow.set_tag(\"valid-data-path\", f'{raw_data_path}/green_tripdata_2023-02.parquet')\n",
    "        mlflow.set_tag(\"test-data-path\",  f'{raw_data_path}/green_tripdata_2023-03.parquet') \n",
    "        \n",
    "        # Set Model params information\n",
    "        best_params = {\n",
    "            \"learning_rate\": 0.09585355369315604,\n",
    "            \"max_depth\": 30,\n",
    "            \"min_child_weight\": 1.060597050922164,\n",
    "            'objective': 'reg:squarederror',          # deprecated  \"reg:linear\"\n",
    "            # 'objective': \"reg:linear\",\n",
    "            \"reg_alpha\": 0.018060244040060163,\n",
    "            \"reg_lambda\": 0.011658731377413597,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        # Build Model   \n",
    "        booster = xgb.train(\n",
    "            params               = best_params,\n",
    "            dtrain               = train,\n",
    "            num_boost_round      = 100,\n",
    "            evals                = [(valid, \"validation\")],\n",
    "            early_stopping_rounds=20,\n",
    "        )   \n",
    "        \n",
    "        # Set Model Evaluation Metric\n",
    "        y_pred = booster.predict(valid)\n",
    "        rmse   = mean_squared_error(y_val, y_pred, squared=False)\n",
    "        mlflow.log_metric(\"rmse\", rmse)       \n",
    "\n",
    "        # Log Model two options\n",
    "        # Option1: Just log model\n",
    "        mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")        \n",
    "        \n",
    "        # Option 2: save Model, Optional: Preprocessor or Pipeline         \n",
    "        # Create dest_path folder unless it already exists\n",
    "        # pathlib.Path(dest_path).mkdir(exist_ok=True) \n",
    "        os.makedirs(dest_path, exist_ok=True)       \n",
    "        local_file = os.path.join(dest_path, \"preprocessor.b\")\n",
    "        with open(local_file, \"wb\") as f_out:\n",
    "            pickle.dump(dv, f_out)\n",
    "            \n",
    "        # whole proccess like pickle, saved Model, Optional: Preprocessor or Pipeline\n",
    "        mlflow.log_artifact(local_path = local_file, artifact_path=\"preprocessor\")        \n",
    "        \n",
    "        # print(f\"default artifacts URI: '{mlflow.get_artifact_uri()}'\")\n",
    "    return None\n",
    "                \n",
    "\n",
    "# click work on Local but it Gives ERRORS Deploy step\n",
    "# @click.command()\n",
    "# @click.option(\n",
    "#     \"--raw_data_path\",\n",
    "#     default=\"./data\",\n",
    "#     help=\"Location where the raw NYC taxi trip data was saved\"\n",
    "# )\n",
    "# @click.option(\n",
    "#     \"--dest_path\",\n",
    "#     default=\"./models\",\n",
    "#     help=\"Location where the resulting model files will be saved\"\n",
    "# )\n",
    "# @click.option(\n",
    "#     \"--years\",\n",
    "#     default=\"2023\",\n",
    "#     help=\"Years where the raw NYC taxi trip data was saved (space-separated)\"\n",
    "# )\n",
    "# @click.option(\n",
    "#     \"--months\",\n",
    "#     default=\"1 2 3 4\",\n",
    "#     help=\"Months where the raw NYC taxi trip data was saved (space-separated)\"\n",
    "# )\n",
    "# @click.option(\n",
    "#     \"--colors\",\n",
    "#     default=\"green yellow\",\n",
    "#     help=\"Colors where the raw NYC taxi trip data was saved\"\n",
    "# )\n",
    "@flow(name=\"Main Flow\")\n",
    "def main_flow(raw_data_path=\"./data\", dest_path=\"./models\", years=\"2023\", months=\"1 2 3 4\", colors=\"green yellow\") -> None:\n",
    "    \"\"\"The main training pipeline\"\"\"    \n",
    "    # Download data    \n",
    "    years  = [int(year) for year in years.split()]\n",
    "    months = [int(month) for month in months.split()]\n",
    "    colors = colors.split()[:1]\n",
    "    download_data(raw_data_path, years, months, colors)\n",
    "    # print(sorted(glob(f'{raw_data_path}/*')))\n",
    "    \n",
    "    # list parquet files\n",
    "    # print(sorted(glob(f'{raw_data_path}/green*.parquet')))\n",
    "    train_path, val_path, test_path = sorted(glob(f'{raw_data_path}/*.parquet'))[:3:]\n",
    "\n",
    "    # Read parquet files\n",
    "    df_train = read_data(train_path)\n",
    "    df_val   = read_data(val_path)\n",
    "    df_test  = read_data(test_path)\n",
    "    # print(df_train.shape, df_val.shape, df_test.shape, )    \n",
    "\n",
    "    # Fit the DictVectorizer and preprocess data\n",
    "    (X_train, y_train), dv = preprocess(df_train, fit_dv=True)\n",
    "    (X_val, y_val)    , _  = preprocess(df_val, dv, fit_dv=False)\n",
    "    (X_test, y_test)  , _  = preprocess(df_test, dv, fit_dv=False)\n",
    "\n",
    "    # Train\n",
    "    train_best_model(X_train, X_val, y_train, y_val, dv, raw_data_path, dest_path)\n",
    "    return None   \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # argparse work on Local but it Gives ERRORS Deploy step\n",
    "    # parser = argparse.ArgumentParser(description=\"Main Flow\")\n",
    "    # parser.add_argument(\"--raw_data_path\", default=\"./data\",       help=\"Location where the raw NYC taxi trip data was saved\")\n",
    "    # parser.add_argument(\"--dest_path\",     default=\"./models\",     help=\"Location where the resulting model files will be saved\")\n",
    "    # parser.add_argument(\"--years\",         default=\"2023\",         help=\"Years where the raw NYC taxi trip data was saved (space-separated)\")\n",
    "    # parser.add_argument(\"--months\",        default=\"1 2 3 4\",      help=\"Months where the raw NYC taxi trip data was saved (space-separated)\")\n",
    "    # parser.add_argument(\"--colors\",        default=\"green yellow\", help=\"Colors where the raw NYC taxi trip data was saved\")\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    # main_flow(args.raw_data_path, args.dest_path, args.years, args.months, args.colors)\n",
    "\n",
    "    main_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f85a3b6a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:12:40.709 | INFO    | prefect.engine - Created flow run 'smiling-worm' for flow 'Main Flow'\n",
      "16:12:41.441 | INFO    | Flow run 'smiling-worm' - Created subflow run 'quaint-flamingo' for flow 'Download Datas'\n",
      "16:12:41.641 | INFO    | Flow run 'quaint-flamingo' - Created task run 'Fetch Data-0' for task 'Fetch Data'\n",
      "16:12:41.644 | INFO    | Flow run 'quaint-flamingo' - Executing 'Fetch Data-0' immediately...\n",
      "16:12:42.358 | INFO    | Task run 'Fetch Data-0' - Finished in state Completed()\n",
      "16:12:42.419 | INFO    | Flow run 'quaint-flamingo' - Created task run 'Fetch Data-1' for task 'Fetch Data'\n",
      "16:12:42.421 | INFO    | Flow run 'quaint-flamingo' - Executing 'Fetch Data-1' immediately...\n",
      "16:12:43.110 | INFO    | Task run 'Fetch Data-1' - Finished in state Completed()\n",
      "16:12:43.177 | INFO    | Flow run 'quaint-flamingo' - Created task run 'Fetch Data-2' for task 'Fetch Data'\n",
      "16:12:43.180 | INFO    | Flow run 'quaint-flamingo' - Executing 'Fetch Data-2' immediately...\n",
      "16:12:43.757 | INFO    | Task run 'Fetch Data-2' - Finished in state Completed()\n",
      "16:12:43.820 | INFO    | Flow run 'quaint-flamingo' - Created task run 'Fetch Data-3' for task 'Fetch Data'\n",
      "16:12:43.822 | INFO    | Flow run 'quaint-flamingo' - Executing 'Fetch Data-3' immediately...\n",
      "16:12:44.503 | INFO    | Task run 'Fetch Data-3' - Finished in state Completed()\n",
      "16:12:44.627 | INFO    | Flow run 'quaint-flamingo' - Finished in state Completed('All states completed.')\n",
      "16:12:44.691 | INFO    | Flow run 'smiling-worm' - Created task run 'Read Data-0' for task 'Read Data'\n",
      "16:12:44.693 | INFO    | Flow run 'smiling-worm' - Executing 'Read Data-0' immediately...\n",
      "16:12:45.400 | INFO    | Task run 'Read Data-0' - Finished in state Completed()\n",
      "16:12:45.459 | INFO    | Flow run 'smiling-worm' - Created task run 'Read Data-1' for task 'Read Data'\n",
      "16:12:45.462 | INFO    | Flow run 'smiling-worm' - Executing 'Read Data-1' immediately...\n",
      "16:12:46.272 | INFO    | Task run 'Read Data-1' - Finished in state Completed()\n",
      "16:12:46.331 | INFO    | Flow run 'smiling-worm' - Created task run 'Read Data-2' for task 'Read Data'\n",
      "16:12:46.334 | INFO    | Flow run 'smiling-worm' - Executing 'Read Data-2' immediately...\n",
      "16:12:47.116 | INFO    | Task run 'Read Data-2' - Finished in state Completed()\n",
      "16:12:47.185 | INFO    | Flow run 'smiling-worm' - Created task run 'Preprocess: Add Features-0' for task 'Preprocess: Add Features'\n",
      "16:12:47.188 | INFO    | Flow run 'smiling-worm' - Executing 'Preprocess: Add Features-0' immediately...\n",
      "16:12:47.962 | INFO    | Task run 'Preprocess: Add Features-0' - Finished in state Completed()\n",
      "16:12:48.026 | INFO    | Flow run 'smiling-worm' - Created task run 'Preprocess: Add Features-1' for task 'Preprocess: Add Features'\n",
      "16:12:48.028 | INFO    | Flow run 'smiling-worm' - Executing 'Preprocess: Add Features-1' immediately...\n",
      "16:12:48.742 | INFO    | Task run 'Preprocess: Add Features-1' - Finished in state Completed()\n",
      "16:12:48.808 | INFO    | Flow run 'smiling-worm' - Created task run 'Preprocess: Add Features-2' for task 'Preprocess: Add Features'\n",
      "16:12:48.811 | INFO    | Flow run 'smiling-worm' - Executing 'Preprocess: Add Features-2' immediately...\n",
      "16:12:49.611 | INFO    | Task run 'Preprocess: Add Features-2' - Finished in state Completed()\n",
      "16:12:49.672 | INFO    | Flow run 'smiling-worm' - Created task run 'Train Best Model-0' for task 'Train Best Model'\n",
      "16:12:49.674 | INFO    | Flow run 'smiling-worm' - Executing 'Train Best Model-0' immediately...\n",
      "2023/08/07 16:12:50 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2023/08/07 16:12:50 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\n",
      "INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "INFO  [alembic.runtime.migration] Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "INFO  [alembic.runtime.migration] Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "INFO  [alembic.runtime.migration] Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2023/08/07 16:12:51 INFO mlflow.tracking.fluent: Experiment with name 'nyc-taxi-experiment' does not exist. Creating a new experiment.\n",
      "INFO  [prefect.task_runs] [0]\tvalidation-rmse:15.01627\n",
      "INFO  [prefect.task_runs] [1]\tvalidation-rmse:13.77591\n",
      "INFO  [prefect.task_runs] [2]\tvalidation-rmse:12.66953\n",
      "INFO  [prefect.task_runs] [3]\tvalidation-rmse:11.68972\n",
      "INFO  [prefect.task_runs] [4]\tvalidation-rmse:10.81927\n",
      "INFO  [prefect.task_runs] [5]\tvalidation-rmse:10.05352\n",
      "INFO  [prefect.task_runs] [6]\tvalidation-rmse:9.37638\n",
      "INFO  [prefect.task_runs] [7]\tvalidation-rmse:8.78514\n",
      "INFO  [prefect.task_runs] [8]\tvalidation-rmse:8.26684\n",
      "INFO  [prefect.task_runs] [9]\tvalidation-rmse:7.81889\n",
      "INFO  [prefect.task_runs] [10]\tvalidation-rmse:7.42994\n",
      "INFO  [prefect.task_runs] [11]\tvalidation-rmse:7.09348\n",
      "INFO  [prefect.task_runs] [12]\tvalidation-rmse:6.80836\n",
      "INFO  [prefect.task_runs] [13]\tvalidation-rmse:6.56211\n",
      "INFO  [prefect.task_runs] [14]\tvalidation-rmse:6.35332\n",
      "INFO  [prefect.task_runs] [15]\tvalidation-rmse:6.17838\n",
      "INFO  [prefect.task_runs] [16]\tvalidation-rmse:6.02763\n",
      "INFO  [prefect.task_runs] [17]\tvalidation-rmse:5.90232\n",
      "INFO  [prefect.task_runs] [18]\tvalidation-rmse:5.79691\n",
      "INFO  [prefect.task_runs] [19]\tvalidation-rmse:5.70817\n",
      "INFO  [prefect.task_runs] [20]\tvalidation-rmse:5.63182\n",
      "INFO  [prefect.task_runs] [21]\tvalidation-rmse:5.56852\n",
      "INFO  [prefect.task_runs] [22]\tvalidation-rmse:5.51520\n",
      "INFO  [prefect.task_runs] [23]\tvalidation-rmse:5.47115\n",
      "INFO  [prefect.task_runs] [24]\tvalidation-rmse:5.43421\n",
      "INFO  [prefect.task_runs] [25]\tvalidation-rmse:5.40206\n",
      "INFO  [prefect.task_runs] [26]\tvalidation-rmse:5.37653\n",
      "INFO  [prefect.task_runs] [27]\tvalidation-rmse:5.35308\n",
      "INFO  [prefect.task_runs] [28]\tvalidation-rmse:5.33437\n",
      "INFO  [prefect.task_runs] [29]\tvalidation-rmse:5.31842\n",
      "INFO  [prefect.task_runs] [30]\tvalidation-rmse:5.30434\n",
      "INFO  [prefect.task_runs] [31]\tvalidation-rmse:5.29293\n",
      "INFO  [prefect.task_runs] [32]\tvalidation-rmse:5.28270\n",
      "INFO  [prefect.task_runs] [33]\tvalidation-rmse:5.27307\n",
      "INFO  [prefect.task_runs] [34]\tvalidation-rmse:5.26638\n",
      "INFO  [prefect.task_runs] [35]\tvalidation-rmse:5.26043\n",
      "INFO  [prefect.task_runs] [36]\tvalidation-rmse:5.25512\n",
      "INFO  [prefect.task_runs] [37]\tvalidation-rmse:5.25088\n",
      "INFO  [prefect.task_runs] [38]\tvalidation-rmse:5.24664\n",
      "INFO  [prefect.task_runs] [39]\tvalidation-rmse:5.24209\n",
      "INFO  [prefect.task_runs] [40]\tvalidation-rmse:5.23909\n",
      "INFO  [prefect.task_runs] [41]\tvalidation-rmse:5.23607\n",
      "INFO  [prefect.task_runs] [42]\tvalidation-rmse:5.23344\n",
      "INFO  [prefect.task_runs] [43]\tvalidation-rmse:5.23200\n",
      "INFO  [prefect.task_runs] [44]\tvalidation-rmse:5.23097\n",
      "INFO  [prefect.task_runs] [45]\tvalidation-rmse:5.22865\n",
      "INFO  [prefect.task_runs] [46]\tvalidation-rmse:5.22775\n",
      "INFO  [prefect.task_runs] [47]\tvalidation-rmse:5.22687\n",
      "INFO  [prefect.task_runs] [48]\tvalidation-rmse:5.22582\n",
      "INFO  [prefect.task_runs] [49]\tvalidation-rmse:5.22487\n",
      "INFO  [prefect.task_runs] [50]\tvalidation-rmse:5.22442\n",
      "INFO  [prefect.task_runs] [51]\tvalidation-rmse:5.22332\n",
      "INFO  [prefect.task_runs] [52]\tvalidation-rmse:5.22236\n",
      "INFO  [prefect.task_runs] [53]\tvalidation-rmse:5.22142\n",
      "INFO  [prefect.task_runs] [54]\tvalidation-rmse:5.22022\n",
      "INFO  [prefect.task_runs] [55]\tvalidation-rmse:5.21987\n",
      "INFO  [prefect.task_runs] [56]\tvalidation-rmse:5.21908\n",
      "INFO  [prefect.task_runs] [57]\tvalidation-rmse:5.21873\n",
      "INFO  [prefect.task_runs] [58]\tvalidation-rmse:5.21820\n",
      "INFO  [prefect.task_runs] [59]\tvalidation-rmse:5.21787\n",
      "INFO  [prefect.task_runs] [60]\tvalidation-rmse:5.21702\n",
      "INFO  [prefect.task_runs] [61]\tvalidation-rmse:5.21632\n",
      "INFO  [prefect.task_runs] [62]\tvalidation-rmse:5.21571\n",
      "INFO  [prefect.task_runs] [63]\tvalidation-rmse:5.21404\n",
      "INFO  [prefect.task_runs] [64]\tvalidation-rmse:5.21348\n",
      "INFO  [prefect.task_runs] [65]\tvalidation-rmse:5.21290\n",
      "INFO  [prefect.task_runs] [66]\tvalidation-rmse:5.21262\n",
      "INFO  [prefect.task_runs] [67]\tvalidation-rmse:5.21181\n",
      "INFO  [prefect.task_runs] [68]\tvalidation-rmse:5.21179\n",
      "INFO  [prefect.task_runs] [69]\tvalidation-rmse:5.21113\n",
      "INFO  [prefect.task_runs] [70]\tvalidation-rmse:5.21051\n",
      "INFO  [prefect.task_runs] [71]\tvalidation-rmse:5.21000\n",
      "INFO  [prefect.task_runs] [72]\tvalidation-rmse:5.20944\n",
      "INFO  [prefect.task_runs] [73]\tvalidation-rmse:5.20928\n",
      "INFO  [prefect.task_runs] [74]\tvalidation-rmse:5.20890\n",
      "INFO  [prefect.task_runs] [75]\tvalidation-rmse:5.20840\n",
      "INFO  [prefect.task_runs] [76]\tvalidation-rmse:5.20723\n",
      "INFO  [prefect.task_runs] [77]\tvalidation-rmse:5.20677\n",
      "INFO  [prefect.task_runs] [78]\tvalidation-rmse:5.20653\n",
      "INFO  [prefect.task_runs] [79]\tvalidation-rmse:5.20608\n",
      "INFO  [prefect.task_runs] [80]\tvalidation-rmse:5.20599\n",
      "INFO  [prefect.task_runs] [81]\tvalidation-rmse:5.20558\n",
      "INFO  [prefect.task_runs] [82]\tvalidation-rmse:5.20516\n",
      "INFO  [prefect.task_runs] [83]\tvalidation-rmse:5.20428\n",
      "INFO  [prefect.task_runs] [84]\tvalidation-rmse:5.20377\n",
      "INFO  [prefect.task_runs] [85]\tvalidation-rmse:5.20323\n",
      "INFO  [prefect.task_runs] [86]\tvalidation-rmse:5.20281\n",
      "INFO  [prefect.task_runs] [87]\tvalidation-rmse:5.20294\n",
      "INFO  [prefect.task_runs] [88]\tvalidation-rmse:5.20271\n",
      "INFO  [prefect.task_runs] [89]\tvalidation-rmse:5.20245\n",
      "INFO  [prefect.task_runs] [90]\tvalidation-rmse:5.20201\n",
      "INFO  [prefect.task_runs] [91]\tvalidation-rmse:5.20186\n",
      "INFO  [prefect.task_runs] [92]\tvalidation-rmse:5.20146\n",
      "INFO  [prefect.task_runs] [93]\tvalidation-rmse:5.20144\n",
      "INFO  [prefect.task_runs] [94]\tvalidation-rmse:5.20096\n",
      "INFO  [prefect.task_runs] [95]\tvalidation-rmse:5.20087\n",
      "INFO  [prefect.task_runs] [96]\tvalidation-rmse:5.20016\n",
      "INFO  [prefect.task_runs] [97]\tvalidation-rmse:5.19983\n",
      "INFO  [prefect.task_runs] [98]\tvalidation-rmse:5.19931\n",
      "INFO  [prefect.task_runs] [99]\tvalidation-rmse:5.19931\n",
      "INFO  [prefect.task_runs] Finished in state Completed()\n",
      "INFO  [prefect.flow_runs] Finished in state Completed('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "# run below file then execute mlflow ui code in terminal\n",
    "!python ./pycode/orchestrate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "603b8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check \n",
    "# prefect server start\n",
    "# prefect deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c088004f",
   "metadata": {},
   "source": [
    "```sh\n",
    "Deployment configuration saved to prefect.yaml! You can now deploy using this deployment configuration with:\n",
    "\n",
    "        $ prefect deploy -n deployment_mlops_zoom\n",
    "\n",
    "You can also make changes to this deployment configuration by making changes to the prefect.yaml file.\n",
    "\n",
    "To execute flow runs from this deployment, start a worker in a separate terminal that pulls work from the 'pool_mlops_zoom' work pool:\n",
    "\n",
    "        $ prefect worker start --pool 'pool_mlops_zoom'\n",
    "\n",
    "To schedule a run for this deployment, use the following command:\n",
    "\n",
    "        $ prefect deployment run 'Main Flow/deployment_mlops_zoom'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca16987e",
   "metadata": {},
   "source": [
    "### OPTIONAL\n",
    "\n",
    "#### **Create a Project in a New Directory**\n",
    "- using the command-line interface (CLI) Select any git-local-s3\n",
    "```sh\n",
    "# follow the Prefect CLI instructions\n",
    "prefect init\n",
    "prefect project init # deprecated\n",
    "```\n",
    "- add another pools\n",
    "```sh\n",
    "prefect block ls\n",
    "prefect block type ls\n",
    "prefect block register -m prefect_aws \n",
    "```\n",
    "\n",
    "#### The base structure for prefect.yaml is as follows:\n",
    "\n",
    "```yaml\n",
    "# generic metadata\n",
    "prefect-version: null\n",
    "name: null\n",
    "\n",
    "# preparation steps\n",
    "build: null\n",
    "push: null\n",
    "\n",
    "# runtime steps\n",
    "pull: null\n",
    "\n",
    "# deployment configurations\n",
    "deployments:\n",
    "  - # base metadata\n",
    "    name: null\n",
    "    version: null\n",
    "    tags: []\n",
    "    description: null\n",
    "    schedule: null\n",
    "\n",
    "    # flow-specific fields\n",
    "    flow_name: null\n",
    "    entrypoint: null\n",
    "    parameters: {}\n",
    "\n",
    "    # infra-specific fields\n",
    "    work_pool:\n",
    "      name: null\n",
    "      work_queue_name: null\n",
    "      job_variables: {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada2c5e",
   "metadata": {},
   "source": [
    "## Q4. RMSE (Add Markdown Artifact)\n",
    "\n",
    "Download the February 2023 Green Taxi data and use it for your training data.\n",
    "Download the March 2023 Green Taxi data and use it for your validation data. \n",
    "\n",
    "Create a Prefect Markdown artifact that displays the RMSE for the validation data.\n",
    "Create a deployment and run it.\n",
    "\n",
    "What’s the RMSE in the artifact to two decimal places ?\n",
    "\n",
    "- 9.71\n",
    "- 12.02\n",
    "- 15.33\n",
    "- 5.37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8dec153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./pycode/create_s3_bucket_block.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/create_s3_bucket_block.py\n",
    "\n",
    "from time import sleep\n",
    "from prefect_aws import S3Bucket, AwsCredentials\n",
    "\n",
    "\n",
    "def create_aws_creds_block():\n",
    "    my_aws_creds_obj = AwsCredentials(\n",
    "        aws_access_key_id=\"123abc\", aws_secret_access_key=\"abc123\"\n",
    "    )\n",
    "    my_aws_creds_obj.save(name=\"my-aws-creds\", overwrite=True)\n",
    "\n",
    "\n",
    "def create_s3_bucket_block():\n",
    "    aws_creds = AwsCredentials.load(\"my-aws-creds\")\n",
    "    my_s3_bucket_obj = S3Bucket(\n",
    "        bucket_name=\"my-first-bucket-abc\", credentials=aws_creds\n",
    "    )\n",
    "    my_s3_bucket_obj.save(name=\"s3-bucket-example\", overwrite=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_aws_creds_block()\n",
    "    sleep(5)\n",
    "    create_s3_bucket_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c76e1e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pycode/orchestrate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pycode/orchestrate.py\n",
    "\n",
    "# Source: https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/03-orchestration/3.4/orchestrate.py\n",
    "\n",
    "import os\n",
    "import click\n",
    "import pickle\n",
    "\n",
    "import pathlib\n",
    "import argparse\n",
    "import requests\n",
    "import urllib.request\n",
    "from glob import glob\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "import mlflow\n",
    "import prefect\n",
    "from prefect import task, flow\n",
    "from prefect.tasks import task_input_hash\n",
    "from prefect.artifacts import create_markdown_artifact\n",
    "\n",
    "# from prefect_aws import S3Bucket\n",
    "# from prefect_email import EmailServerCredentials, email_send_message\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# Filter the specific warning message, MLflow autologging encountered a warning\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"setuptools\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Setuptools is replacing distutils.\")\n",
    "\n",
    "\n",
    "@task(name=\"Fetch Data\", cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1),\n",
    "      retries=3, log_prints=True, )\n",
    "def fetch_data(raw_data_path: str, year: int, month: int, color: str) -> None:\n",
    "    \"\"\"Fetches data from the NYC Taxi dataset and saves it locally\"\"\"\n",
    "    url      = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{color}_tripdata_{year}-{month:0>2}.parquet'    \n",
    "    filename = os.path.join(raw_data_path, f'{color}_tripdata_{year}-{month:0>2}.parquet')\n",
    "\n",
    "    # Create dest_path folder unless it already exists\n",
    "    os.makedirs(raw_data_path, exist_ok=True)\n",
    "    \n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    # os.system(f\"wget -q -N -P {raw_data_path} {url}\")\n",
    "    # urllib.request.urlretrieve(url, filename)\n",
    "    response = requests.get(url)\n",
    "    with open(filename, \"wb\") as f_out:\n",
    "        f_out.write(response.content)\n",
    "    return None\n",
    "\n",
    "\n",
    "@flow(name=\"Download Datas\", log_prints=True)\n",
    "def download_data(raw_data_path: str, years: list, months: list, colors: list) -> None:\n",
    "    # Download the data from the NYC Taxi dataset\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            for color in colors:\n",
    "                fetch_data(raw_data_path, year, month, color)\n",
    "    return None\n",
    "    \n",
    "    \n",
    "@task(name=\"Read Data\", retries=3, retry_delay_seconds=2, log_prints=None)\n",
    "def read_data(filename: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "    df.lpep_pickup_datetime  = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "\n",
    "    df[\"duration\"] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df[\"duration\"] = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical     = [\"PULocationID\", \"DOLocationID\"]\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "@task(name=\"Preprocess: Add Features\", log_prints=True)\n",
    "def preprocess(\n",
    "    df: pd.DataFrame,dv: DictVectorizer = None, fit_dv: bool = False\n",
    ") -> tuple(\n",
    "    [\n",
    "        scipy.sparse._csr.csr_matrix,\n",
    "        np.ndarray,\n",
    "        sklearn.feature_extraction.DictVectorizer,\n",
    "    ]\n",
    "):\n",
    "    \"\"\"Add features to the model\"\"\"\n",
    "    df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']\n",
    "    categorical = [\"PU_DO\"]\n",
    "    numerical   = ['trip_distance']\n",
    "    dicts       = df[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    if fit_dv:\n",
    "        # return sparse matrix\n",
    "        dv = DictVectorizer()\n",
    "        X = dv.fit_transform(dicts)\n",
    "    else:\n",
    "        X = dv.transform(dicts)\n",
    "        \n",
    "    # Convert X the sparse matrix  to pandas DataFrame, but too slow\n",
    "    # X = pd.DataFrame(X.toarray(), columns=dv.get_feature_names_out())\n",
    "    # X = pd.DataFrame.sparse.from_spmatrix(X, columns=dv.get_feature_names_out())\n",
    "\n",
    "    try:\n",
    "        # Extract the target\n",
    "        target = 'duration'\n",
    "        y = df[target].values\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return (X, y), dv\n",
    "\n",
    "\n",
    "@task(name=\"Train Best Model\", log_prints=True)\n",
    "def train_best_model(\n",
    "    X_train  : scipy.sparse._csr.csr_matrix,\n",
    "    X_val    : scipy.sparse._csr.csr_matrix,\n",
    "    y_train  : np.ndarray,\n",
    "    y_val    : np.ndarray,\n",
    "    dv       : sklearn.feature_extraction.DictVectorizer,\n",
    "    raw_data_path: str,\n",
    "    dest_path: str,\n",
    ") -> None:\n",
    "    \"\"\"train a model with best hyperparams and write everything out\"\"\"        \n",
    "    # Load train and test Data\n",
    "    train = xgb.DMatrix(X_train, label=y_train)\n",
    "    valid = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    # MLflow settings\n",
    "    # Build or Connect Database Offline\n",
    "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "    # Connect Database Online\n",
    "    # mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "    \n",
    "    # Build or Connect mlflow experiment\n",
    "    EXPERIMENT_NAME = \"nyc-taxi-experiment\"\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "    # before your training code to enable automatic logging of sklearn metrics, params, and models\n",
    "    # mlflow.xgboost.autolog()\n",
    "    \n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Optional: Set some information about Model\n",
    "        mlflow.set_tag(\"developer\", \"muce\")\n",
    "        mlflow.set_tag(\"algorithm\", \"Machine Learning\")\n",
    "        mlflow.set_tag(\"train-data-path\", f'{raw_data_path}/green_tripdata_2023-01.parquet')\n",
    "        mlflow.set_tag(\"valid-data-path\", f'{raw_data_path}/green_tripdata_2023-02.parquet')\n",
    "        mlflow.set_tag(\"test-data-path\",  f'{raw_data_path}/green_tripdata_2023-03.parquet') \n",
    "        \n",
    "        # Set Model params information\n",
    "        best_params = {\n",
    "            \"learning_rate\": 0.09585355369315604,\n",
    "            \"max_depth\": 30,\n",
    "            \"min_child_weight\": 1.060597050922164,\n",
    "            'objective': 'reg:squarederror',          # deprecated  \"reg:linear\"\n",
    "            # 'objective': \"reg:linear\",\n",
    "            \"reg_alpha\": 0.018060244040060163,\n",
    "            \"reg_lambda\": 0.011658731377413597,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        # Build Model   \n",
    "        booster = xgb.train(\n",
    "            params               = best_params,\n",
    "            dtrain               = train,\n",
    "            num_boost_round      = 100,\n",
    "            evals                = [(valid, \"validation\")],\n",
    "            early_stopping_rounds=20,\n",
    "        )   \n",
    "        \n",
    "        # Set Model Evaluation Metric\n",
    "        y_pred = booster.predict(valid)\n",
    "        rmse   = mean_squared_error(y_val, y_pred, squared=False)\n",
    "        mlflow.log_metric(\"rmse\", rmse)       \n",
    "\n",
    "        # Log Model two options\n",
    "        # Option1: Just log model\n",
    "        mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")        \n",
    "        \n",
    "        # Option 2: save Model, Optional: Preprocessor or Pipeline         \n",
    "        # Create dest_path folder unless it already exists\n",
    "        # pathlib.Path(dest_path).mkdir(exist_ok=True) \n",
    "        os.makedirs(dest_path, exist_ok=True)       \n",
    "        local_file = os.path.join(dest_path, \"preprocessor.b\")\n",
    "        with open(local_file, \"wb\") as f_out:\n",
    "            pickle.dump(dv, f_out)\n",
    "            \n",
    "        # whole proccess like pickle, saved Model, Optional: Preprocessor or Pipeline\n",
    "        mlflow.log_artifact(local_path = local_file, artifact_path=\"preprocessor\")        \n",
    "        \n",
    "        # print(f\"default artifacts URI: '{mlflow.get_artifact_uri()}'\")\n",
    "\n",
    "        # Add markdown artifact with RMSE value\n",
    "        markdown__rmse_report = f\"\"\"\n",
    "# RMSE Report\n",
    "\n",
    "## Summary\n",
    "\n",
    "Duration Prediction \n",
    "\n",
    "## RMSE XGBoost Model\n",
    "\n",
    "| Region    | RMSE |\n",
    "|:----------|-------:|\n",
    "| {date.today()} | {rmse:.2f} |\n",
    "\"\"\"\n",
    "        create_markdown_artifact(\n",
    "            key=\"duration-model-report\", \n",
    "            markdown=markdown__rmse_report,\n",
    "            description=\"RMSE for Validation Data Report\",\n",
    "        )\n",
    "    return None\n",
    "\n",
    "\n",
    "# @flow(name=\"Email Server Crenditals\", log_prints=True)\n",
    "# def example_email_send_message_flow(email_addresses: list[str]):\n",
    "#     email_server_credentials = EmailServerCredentials.load(\"email-server-credentials\")\n",
    "    \n",
    "#     for email_address in email_addresses:\n",
    "#         subject = email_send_message.with_options(name=f\"email {email_address}\").submit(\n",
    "#             email_server_credentials=email_server_credentials,\n",
    "#             subject=\"Example Flow Notification using Gmail\",\n",
    "#             msg=\"This proves email_send_message works!\",\n",
    "#             email_to=email_address,\n",
    "#         )\n",
    "                \n",
    "\n",
    "# click work on Local but it Gives ERRORS Deploy step\n",
    "# @click.command()\n",
    "# @click.option(\n",
    "#     \"--raw_data_path\",\n",
    "#     default=\"./data\",\n",
    "#     help=\"Location where the raw NYC taxi trip data was saved\"\n",
    "# )\n",
    "# @click.option(\n",
    "#     \"--dest_path\",\n",
    "#     default=\"./models\",\n",
    "#     help=\"Location where the resulting model files will be saved\"\n",
    "# )\n",
    "# @click.option(\n",
    "#     \"--years\",\n",
    "#     default=\"2023\",\n",
    "#     help=\"Years where the raw NYC taxi trip data was saved (space-separated)\"\n",
    "# )\n",
    "# @click.option(\n",
    "#     \"--months\",\n",
    "#     default=\"1 2 3 4\",\n",
    "#     help=\"Months where the raw NYC taxi trip data was saved (space-separated)\"\n",
    "# )\n",
    "# @click.option(\n",
    "#     \"--colors\",\n",
    "#     default=\"green yellow\",\n",
    "#     help=\"Colors where the raw NYC taxi trip data was saved\"\n",
    "# )\n",
    "@flow(name=\"Main Flow\")\n",
    "def main_flow(raw_data_path=\"./data\", dest_path=\"./models\", years=\"2023\", months=\"1 2 3 4\", colors=\"green yellow\") -> None:\n",
    "    \"\"\"The main training pipeline\"\"\"    \n",
    "    # Download data    \n",
    "    years  = [int(year) for year in years.split()]\n",
    "    months = [int(month) for month in months.split()]\n",
    "    colors = colors.split()[:1]\n",
    "    download_data(raw_data_path, years, months, colors)\n",
    "    # print(sorted(glob(f'{raw_data_path}/*')))\n",
    "    \n",
    "    # list parquet files\n",
    "    # print(sorted(glob(f'{raw_data_path}/green*.parquet')))\n",
    "    train_path, val_path, test_path = sorted(glob(f'{raw_data_path}/*.parquet'))[:3:]\n",
    "\n",
    "    # Read parquet files\n",
    "    df_train = read_data(train_path)\n",
    "    df_val   = read_data(val_path)\n",
    "    df_test  = read_data(test_path)\n",
    "    # print(df_train.shape, df_val.shape, df_test.shape, )    \n",
    "\n",
    "    # Fit the DictVectorizer and preprocess data\n",
    "    (X_train, y_train), dv = preprocess(df_train, fit_dv=True)\n",
    "    (X_val, y_val)    , _  = preprocess(df_val, dv, fit_dv=False)\n",
    "    (X_test, y_test)  , _  = preprocess(df_test, dv, fit_dv=False)\n",
    "\n",
    "    # Train\n",
    "    train_best_model(X_train, X_val, y_train, y_val, dv, raw_data_path, dest_path)\n",
    "    return None   \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # argparse work on Local but it Gives ERRORS Deploy step\n",
    "    # parser = argparse.ArgumentParser(description=\"Main Flow\")\n",
    "    # parser.add_argument(\"--raw_data_path\", default=\"./data\",       help=\"Location where the raw NYC taxi trip data was saved\")\n",
    "    # parser.add_argument(\"--dest_path\",     default=\"./models\",     help=\"Location where the resulting model files will be saved\")\n",
    "    # parser.add_argument(\"--years\",         default=\"2023\",         help=\"Years where the raw NYC taxi trip data was saved (space-separated)\")\n",
    "    # parser.add_argument(\"--months\",        default=\"1 2 3 4\",      help=\"Months where the raw NYC taxi trip data was saved (space-separated)\")\n",
    "    # parser.add_argument(\"--colors\",        default=\"green yellow\", help=\"Colors where the raw NYC taxi trip data was saved\")\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    # main_flow(args.raw_data_path, args.dest_path, args.years, args.months, args.colors)\n",
    "\n",
    "    main_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b5f353c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:26:03.356 | INFO    | prefect.engine - Created flow run 'diamond-toucan' for flow 'Main Flow'\n",
      "16:26:04.289 | INFO    | Flow run 'diamond-toucan' - Created subflow run 'romantic-husky' for flow 'Download Datas'\n",
      "16:26:04.510 | INFO    | Flow run 'romantic-husky' - Created task run 'Fetch Data-0' for task 'Fetch Data'\n",
      "16:26:04.512 | INFO    | Flow run 'romantic-husky' - Executing 'Fetch Data-0' immediately...\n",
      "16:26:05.389 | INFO    | Task run 'Fetch Data-0' - Finished in state Completed()\n",
      "16:26:05.455 | INFO    | Flow run 'romantic-husky' - Created task run 'Fetch Data-1' for task 'Fetch Data'\n",
      "16:26:05.456 | INFO    | Flow run 'romantic-husky' - Executing 'Fetch Data-1' immediately...\n",
      "16:26:06.026 | INFO    | Task run 'Fetch Data-1' - Finished in state Completed()\n",
      "16:26:06.089 | INFO    | Flow run 'romantic-husky' - Created task run 'Fetch Data-2' for task 'Fetch Data'\n",
      "16:26:06.091 | INFO    | Flow run 'romantic-husky' - Executing 'Fetch Data-2' immediately...\n",
      "16:26:06.786 | INFO    | Task run 'Fetch Data-2' - Finished in state Completed()\n",
      "16:26:06.858 | INFO    | Flow run 'romantic-husky' - Created task run 'Fetch Data-3' for task 'Fetch Data'\n",
      "16:26:06.861 | INFO    | Flow run 'romantic-husky' - Executing 'Fetch Data-3' immediately...\n",
      "16:26:07.476 | INFO    | Task run 'Fetch Data-3' - Finished in state Completed()\n",
      "16:26:07.604 | INFO    | Flow run 'romantic-husky' - Finished in state Completed('All states completed.')\n",
      "16:26:07.664 | INFO    | Flow run 'diamond-toucan' - Created task run 'Read Data-0' for task 'Read Data'\n",
      "16:26:07.666 | INFO    | Flow run 'diamond-toucan' - Executing 'Read Data-0' immediately...\n",
      "16:26:10.449 | INFO    | Task run 'Read Data-0' - Finished in state Completed()\n",
      "16:26:10.508 | INFO    | Flow run 'diamond-toucan' - Created task run 'Read Data-1' for task 'Read Data'\n",
      "16:26:10.511 | INFO    | Flow run 'diamond-toucan' - Executing 'Read Data-1' immediately...\n",
      "16:26:11.359 | INFO    | Task run 'Read Data-1' - Finished in state Completed()\n",
      "16:26:11.431 | INFO    | Flow run 'diamond-toucan' - Created task run 'Read Data-2' for task 'Read Data'\n",
      "16:26:11.434 | INFO    | Flow run 'diamond-toucan' - Executing 'Read Data-2' immediately...\n",
      "16:26:12.280 | INFO    | Task run 'Read Data-2' - Finished in state Completed()\n",
      "16:26:12.350 | INFO    | Flow run 'diamond-toucan' - Created task run 'Preprocess: Add Features-0' for task 'Preprocess: Add Features'\n",
      "16:26:12.352 | INFO    | Flow run 'diamond-toucan' - Executing 'Preprocess: Add Features-0' immediately...\n",
      "16:26:13.509 | INFO    | Task run 'Preprocess: Add Features-0' - Finished in state Completed()\n",
      "16:26:13.579 | INFO    | Flow run 'diamond-toucan' - Created task run 'Preprocess: Add Features-1' for task 'Preprocess: Add Features'\n",
      "16:26:13.581 | INFO    | Flow run 'diamond-toucan' - Executing 'Preprocess: Add Features-1' immediately...\n",
      "16:26:14.347 | INFO    | Task run 'Preprocess: Add Features-1' - Finished in state Completed()\n",
      "16:26:14.412 | INFO    | Flow run 'diamond-toucan' - Created task run 'Preprocess: Add Features-2' for task 'Preprocess: Add Features'\n",
      "16:26:14.415 | INFO    | Flow run 'diamond-toucan' - Executing 'Preprocess: Add Features-2' immediately...\n",
      "16:26:15.272 | INFO    | Task run 'Preprocess: Add Features-2' - Finished in state Completed()\n",
      "16:26:15.338 | INFO    | Flow run 'diamond-toucan' - Created task run 'Train Best Model-0' for task 'Train Best Model'\n",
      "16:26:15.341 | INFO    | Flow run 'diamond-toucan' - Executing 'Train Best Model-0' immediately...\n",
      "16:26:18.088 | INFO    | Task run 'Train Best Model-0' - [0]    validation-rmse:15.01627\n",
      "16:26:18.279 | INFO    | Task run 'Train Best Model-0' - [1]    validation-rmse:13.77591\n",
      "16:26:18.508 | INFO    | Task run 'Train Best Model-0' - [2]    validation-rmse:12.66953\n",
      "16:26:18.719 | INFO    | Task run 'Train Best Model-0' - [3]    validation-rmse:11.68972\n",
      "16:26:18.931 | INFO    | Task run 'Train Best Model-0' - [4]    validation-rmse:10.81927\n",
      "16:26:19.131 | INFO    | Task run 'Train Best Model-0' - [5]    validation-rmse:10.05352\n",
      "16:26:19.346 | INFO    | Task run 'Train Best Model-0' - [6]    validation-rmse:9.37638\n",
      "16:26:19.544 | INFO    | Task run 'Train Best Model-0' - [7]    validation-rmse:8.78514\n",
      "16:26:19.761 | INFO    | Task run 'Train Best Model-0' - [8]    validation-rmse:8.26684\n",
      "16:26:19.980 | INFO    | Task run 'Train Best Model-0' - [9]    validation-rmse:7.81889\n",
      "16:26:20.192 | INFO    | Task run 'Train Best Model-0' - [10]   validation-rmse:7.42994\n",
      "16:26:20.401 | INFO    | Task run 'Train Best Model-0' - [11]   validation-rmse:7.09348\n",
      "16:26:20.627 | INFO    | Task run 'Train Best Model-0' - [12]   validation-rmse:6.80836\n",
      "16:26:20.848 | INFO    | Task run 'Train Best Model-0' - [13]   validation-rmse:6.56211\n",
      "16:26:21.113 | INFO    | Task run 'Train Best Model-0' - [14]   validation-rmse:6.35332\n",
      "16:26:21.306 | INFO    | Task run 'Train Best Model-0' - [15]   validation-rmse:6.17838\n",
      "16:26:21.499 | INFO    | Task run 'Train Best Model-0' - [16]   validation-rmse:6.02763\n",
      "16:26:21.678 | INFO    | Task run 'Train Best Model-0' - [17]   validation-rmse:5.90232\n",
      "16:26:21.856 | INFO    | Task run 'Train Best Model-0' - [18]   validation-rmse:5.79691\n",
      "16:26:22.038 | INFO    | Task run 'Train Best Model-0' - [19]   validation-rmse:5.70817\n",
      "16:26:22.197 | INFO    | Task run 'Train Best Model-0' - [20]   validation-rmse:5.63182\n",
      "16:26:22.360 | INFO    | Task run 'Train Best Model-0' - [21]   validation-rmse:5.56852\n",
      "16:26:22.518 | INFO    | Task run 'Train Best Model-0' - [22]   validation-rmse:5.51520\n",
      "16:26:22.698 | INFO    | Task run 'Train Best Model-0' - [23]   validation-rmse:5.47115\n",
      "16:26:22.889 | INFO    | Task run 'Train Best Model-0' - [24]   validation-rmse:5.43421\n",
      "16:26:23.152 | INFO    | Task run 'Train Best Model-0' - [25]   validation-rmse:5.40206\n",
      "16:26:23.350 | INFO    | Task run 'Train Best Model-0' - [26]   validation-rmse:5.37653\n",
      "16:26:23.507 | INFO    | Task run 'Train Best Model-0' - [27]   validation-rmse:5.35308\n",
      "16:26:23.645 | INFO    | Task run 'Train Best Model-0' - [28]   validation-rmse:5.33437\n",
      "16:26:23.777 | INFO    | Task run 'Train Best Model-0' - [29]   validation-rmse:5.31842\n",
      "16:26:23.921 | INFO    | Task run 'Train Best Model-0' - [30]   validation-rmse:5.30434\n",
      "16:26:24.069 | INFO    | Task run 'Train Best Model-0' - [31]   validation-rmse:5.29293\n",
      "16:26:24.247 | INFO    | Task run 'Train Best Model-0' - [32]   validation-rmse:5.28270\n",
      "16:26:24.401 | INFO    | Task run 'Train Best Model-0' - [33]   validation-rmse:5.27307\n",
      "16:26:24.541 | INFO    | Task run 'Train Best Model-0' - [34]   validation-rmse:5.26638\n",
      "16:26:24.691 | INFO    | Task run 'Train Best Model-0' - [35]   validation-rmse:5.26043\n",
      "16:26:24.829 | INFO    | Task run 'Train Best Model-0' - [36]   validation-rmse:5.25512\n",
      "16:26:24.969 | INFO    | Task run 'Train Best Model-0' - [37]   validation-rmse:5.25088\n",
      "16:26:25.106 | INFO    | Task run 'Train Best Model-0' - [38]   validation-rmse:5.24664\n",
      "16:26:25.255 | INFO    | Task run 'Train Best Model-0' - [39]   validation-rmse:5.24209\n",
      "16:26:25.387 | INFO    | Task run 'Train Best Model-0' - [40]   validation-rmse:5.23909\n",
      "16:26:25.507 | INFO    | Task run 'Train Best Model-0' - [41]   validation-rmse:5.23607\n",
      "16:26:25.630 | INFO    | Task run 'Train Best Model-0' - [42]   validation-rmse:5.23344\n",
      "16:26:25.743 | INFO    | Task run 'Train Best Model-0' - [43]   validation-rmse:5.23200\n",
      "16:26:25.852 | INFO    | Task run 'Train Best Model-0' - [44]   validation-rmse:5.23097\n",
      "16:26:25.965 | INFO    | Task run 'Train Best Model-0' - [45]   validation-rmse:5.22865\n",
      "16:26:26.078 | INFO    | Task run 'Train Best Model-0' - [46]   validation-rmse:5.22775\n",
      "16:26:26.195 | INFO    | Task run 'Train Best Model-0' - [47]   validation-rmse:5.22687\n",
      "16:26:26.340 | INFO    | Task run 'Train Best Model-0' - [48]   validation-rmse:5.22582\n",
      "16:26:26.452 | INFO    | Task run 'Train Best Model-0' - [49]   validation-rmse:5.22487\n",
      "16:26:26.570 | INFO    | Task run 'Train Best Model-0' - [50]   validation-rmse:5.22442\n",
      "16:26:26.698 | INFO    | Task run 'Train Best Model-0' - [51]   validation-rmse:5.22332\n",
      "16:26:26.826 | INFO    | Task run 'Train Best Model-0' - [52]   validation-rmse:5.22236\n",
      "16:26:26.946 | INFO    | Task run 'Train Best Model-0' - [53]   validation-rmse:5.22142\n",
      "16:26:27.061 | INFO    | Task run 'Train Best Model-0' - [54]   validation-rmse:5.22022\n",
      "16:26:27.174 | INFO    | Task run 'Train Best Model-0' - [55]   validation-rmse:5.21987\n",
      "16:26:27.286 | INFO    | Task run 'Train Best Model-0' - [56]   validation-rmse:5.21908\n",
      "16:26:27.417 | INFO    | Task run 'Train Best Model-0' - [57]   validation-rmse:5.21873\n",
      "16:26:27.540 | INFO    | Task run 'Train Best Model-0' - [58]   validation-rmse:5.21820\n",
      "16:26:27.682 | INFO    | Task run 'Train Best Model-0' - [59]   validation-rmse:5.21787\n",
      "16:26:27.795 | INFO    | Task run 'Train Best Model-0' - [60]   validation-rmse:5.21702\n",
      "16:26:27.907 | INFO    | Task run 'Train Best Model-0' - [61]   validation-rmse:5.21632\n",
      "16:26:28.018 | INFO    | Task run 'Train Best Model-0' - [62]   validation-rmse:5.21571\n",
      "16:26:28.132 | INFO    | Task run 'Train Best Model-0' - [63]   validation-rmse:5.21404\n",
      "16:26:28.246 | INFO    | Task run 'Train Best Model-0' - [64]   validation-rmse:5.21348\n",
      "16:26:28.366 | INFO    | Task run 'Train Best Model-0' - [65]   validation-rmse:5.21290\n",
      "16:26:28.501 | INFO    | Task run 'Train Best Model-0' - [66]   validation-rmse:5.21262\n",
      "16:26:28.651 | INFO    | Task run 'Train Best Model-0' - [67]   validation-rmse:5.21181\n",
      "16:26:28.767 | INFO    | Task run 'Train Best Model-0' - [68]   validation-rmse:5.21179\n",
      "16:26:28.889 | INFO    | Task run 'Train Best Model-0' - [69]   validation-rmse:5.21113\n",
      "16:26:29.007 | INFO    | Task run 'Train Best Model-0' - [70]   validation-rmse:5.21051\n",
      "16:26:29.122 | INFO    | Task run 'Train Best Model-0' - [71]   validation-rmse:5.21000\n",
      "16:26:29.232 | INFO    | Task run 'Train Best Model-0' - [72]   validation-rmse:5.20944\n",
      "16:26:29.338 | INFO    | Task run 'Train Best Model-0' - [73]   validation-rmse:5.20928\n",
      "16:26:29.461 | INFO    | Task run 'Train Best Model-0' - [74]   validation-rmse:5.20890\n",
      "16:26:29.579 | INFO    | Task run 'Train Best Model-0' - [75]   validation-rmse:5.20840\n",
      "16:26:29.707 | INFO    | Task run 'Train Best Model-0' - [76]   validation-rmse:5.20723\n",
      "16:26:29.819 | INFO    | Task run 'Train Best Model-0' - [77]   validation-rmse:5.20677\n",
      "16:26:29.933 | INFO    | Task run 'Train Best Model-0' - [78]   validation-rmse:5.20653\n",
      "16:26:30.050 | INFO    | Task run 'Train Best Model-0' - [79]   validation-rmse:5.20608\n",
      "16:26:30.164 | INFO    | Task run 'Train Best Model-0' - [80]   validation-rmse:5.20599\n",
      "16:26:30.275 | INFO    | Task run 'Train Best Model-0' - [81]   validation-rmse:5.20558\n",
      "16:26:30.394 | INFO    | Task run 'Train Best Model-0' - [82]   validation-rmse:5.20516\n",
      "16:26:30.525 | INFO    | Task run 'Train Best Model-0' - [83]   validation-rmse:5.20428\n",
      "16:26:30.637 | INFO    | Task run 'Train Best Model-0' - [84]   validation-rmse:5.20377\n",
      "16:26:30.751 | INFO    | Task run 'Train Best Model-0' - [85]   validation-rmse:5.20323\n",
      "16:26:30.875 | INFO    | Task run 'Train Best Model-0' - [86]   validation-rmse:5.20281\n",
      "16:26:30.988 | INFO    | Task run 'Train Best Model-0' - [87]   validation-rmse:5.20294\n",
      "16:26:31.104 | INFO    | Task run 'Train Best Model-0' - [88]   validation-rmse:5.20271\n",
      "16:26:31.212 | INFO    | Task run 'Train Best Model-0' - [89]   validation-rmse:5.20245\n",
      "16:26:31.324 | INFO    | Task run 'Train Best Model-0' - [90]   validation-rmse:5.20201\n",
      "16:26:31.434 | INFO    | Task run 'Train Best Model-0' - [91]   validation-rmse:5.20186\n",
      "16:26:31.556 | INFO    | Task run 'Train Best Model-0' - [92]   validation-rmse:5.20146\n",
      "16:26:31.672 | INFO    | Task run 'Train Best Model-0' - [93]   validation-rmse:5.20144\n",
      "16:26:31.804 | INFO    | Task run 'Train Best Model-0' - [94]   validation-rmse:5.20096\n",
      "16:26:31.926 | INFO    | Task run 'Train Best Model-0' - [95]   validation-rmse:5.20087\n",
      "16:26:32.039 | INFO    | Task run 'Train Best Model-0' - [96]   validation-rmse:5.20016\n",
      "16:26:32.147 | INFO    | Task run 'Train Best Model-0' - [97]   validation-rmse:5.19983\n",
      "16:26:32.265 | INFO    | Task run 'Train Best Model-0' - [98]   validation-rmse:5.19931\n",
      "16:26:32.395 | INFO    | Task run 'Train Best Model-0' - [99]   validation-rmse:5.19931\n",
      "16:26:49.570 | INFO    | Task run 'Train Best Model-0' - Finished in state Completed()\n",
      "16:26:49.690 | INFO    | Flow run 'diamond-toucan' - Finished in state Completed('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "!python ./pycode/orchestrate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4852bb63",
   "metadata": {},
   "source": [
    "### Prefect Deployment\n",
    "\n",
    "```sh\n",
    "# follow the Prefect CLI instructions\n",
    "prefect init\n",
    "# Created: .prefect, .prefectignore\n",
    "\n",
    "prefect server start\n",
    "\n",
    "# make sure to save the modified code and available data.\n",
    "# push the code GitHub and available data.\n",
    "\n",
    "# recommended: follow the Prefect CLI instructions to create the work pool, then save it.\n",
    "prefect deploy -n deployment_mlops_zoom\n",
    "# Deployment 'Main Flow/deployment_mlops_zoom' successfully created with id '58b4a702-c7d3-44c5-99d8-e12f5091ac7a'.    \n",
    "\n",
    "# Start a worker\n",
    "prefect worker start --pool pool_mlops_zoom\n",
    "# Work pool \"pool_mlops_zoom\" does not exist and no worker type was provided. Starting a process worker...\n",
    "# Worker 'ProcessWorker c42ee57f-51bb-4b4e-9428-c25bad4356a2' started!\n",
    "\n",
    "prefect deployment run \"Main Flow/deployment_mlops_zoom\"\n",
    "# Creating flow run for deployment 'Main Flow/deployment_mlops_zoom'...\n",
    "# Created flow run 'denim-limpet'.\n",
    "# └── UUID: 6e5ea132-c58e-454b-b0ac-eb9b8f5f7afd\n",
    "# └── Parameters: {}\n",
    "# └── Scheduled start time: 2023-08-07 16:31:34 +03 (now)\n",
    "# └── URL: <no dashboard available>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3476a2d",
   "metadata": {},
   "source": [
    "```\n",
    "31f95cf7\n",
    "Flow run\n",
    "wonderful-bat\n",
    "Task run\n",
    "Train Best Model-0\n",
    "RMSE for Validation Data Report\n",
    "RMSE for Validation Data\n",
    "RMSE: 5.374495195206525\n",
    "\n",
    "\n",
    "dc032057\n",
    "Flow run\n",
    "interesting-cougar\n",
    "Task run\n",
    "Train Best Model-0\n",
    "Created gtm-report\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c2cde2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb448527",
   "metadata": {},
   "source": [
    "## Q5. Emails\n",
    "\n",
    "\n",
    "It’s often helpful to be notified when something with your dataflow doesn’t work\n",
    "as planned. Create an email notification for to use with your own Prefect server instance.\n",
    "In your virtual environment, install the prefect-email integration with \n",
    "\n",
    "```bash\n",
    "pip install prefect-email\n",
    "```\n",
    "\n",
    "Make sure you are connected to a running Prefect server instance through your\n",
    "Prefect profile.\n",
    "See the docs if needed: https://docs.prefect.io/latest/concepts/settings/#configuration-profiles\n",
    "\n",
    "Register the new block with your server with \n",
    "\n",
    "```bash\n",
    "prefect block register -m prefect_email\n",
    "```\n",
    "\n",
    "Remember that a block is a Prefect class with a nice UI form interface.\n",
    "Block objects live on the server and can be created and accessed in your Python code. \n",
    "\n",
    "See the docs for how to authenticate by saving your email credentials to\n",
    "a block and note that you will need an App Password to send emails with\n",
    "Gmail and other services. Follow the instructions in the docs.\n",
    "\n",
    "Create and save an `EmailServerCredentials` notification block.\n",
    "Use the credentials block to send an email.\n",
    "\n",
    "Test the notification functionality by running a deployment.\n",
    "\n",
    "What is the name of the pre-built prefect-email task function?\n",
    "\n",
    "- `send_email_message`\n",
    "- `email_send_message`\n",
    "- `send_email`\n",
    "- `send_message`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e209b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSuccessfully registered 1 block\u001b[0m\n",
      "\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mRegistered Blocks       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│ Email Server Credentials │\n",
      "└──────────────────────────┘\n",
      "\n",
      " To configure the newly registered blocks, go to the Blocks page in the Prefect \n",
      "UI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!prefect block register -m prefect_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84767650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object Block.save at 0x7f2de7b45380>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prefect_email import EmailServerCredentials\n",
    "\n",
    "credentials = EmailServerCredentials(\n",
    "    username=\"EMAIL-ADDRESS-PLACEHOLDER\",\n",
    "    password=\"PASSWORD-PLACEHOLDER\",  # must be an app password\n",
    ")\n",
    "credentials.save(\"BLOCK-NAME-PLACEHOLDER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91168fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object Block.load at 0x7f2de7b45620>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prefect_email import EmailServerCredentials\n",
    "\n",
    "EmailServerCredentials.load(\"BLOCK_NAME_PLACEHOLDER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78f441a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Send an email using Gmail\n",
    "from prefect import flow\n",
    "from prefect_email import EmailServerCredentials, email_send_message\n",
    "\n",
    "@flow(name=\"Email Server Credentials\")\n",
    "def example_email_send_message_flow():\n",
    "    email_server_credentials = EmailServerCredentials(\n",
    "        username=\"your_email_address@gmail.com\",\n",
    "        password=\"MUST_be_an_app_password_here!\",\n",
    "    )\n",
    "    # email_server_credentials.save(\"BLOCK-NAME-PLACEHOLDER\")\n",
    "    # email_server_credentials = EmailServerCredentials.load(\"BLOCK_NAME_PLACEHOLDER\")\n",
    "    \n",
    "    subject = email_send_message(\n",
    "        email_server_credentials=email_server_credentials,\n",
    "        subject=\"Example Flow Notification using Gmail\",\n",
    "        msg=\"This proves email_send_message works!\",\n",
    "        email_to=\"someone_awesome@gmail.com\",\n",
    "    )\n",
    "    return subject\n",
    "\n",
    "# example_email_send_message_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53b9c2",
   "metadata": {},
   "source": [
    "## Q6. Prefect Cloud\n",
    "\n",
    "The hosted Prefect Cloud lets you avoid running your own Prefect server and\n",
    "has automations that allow you to get notifications when certain events occur\n",
    "or don’t occur. \n",
    "\n",
    "Create a free forever Prefect Cloud account at [app.prefect.cloud](https://app.prefect.cloud/) and connect\n",
    "your workspace to it following the steps in the UI when you sign up. \n",
    "\n",
    "Set up an Automation from the UI that will send yourself an email when\n",
    "a flow run completes. Run one of your existing deployments and check\n",
    "your email to see the notification.\n",
    "\n",
    "Make sure your active profile is pointing toward Prefect Cloud and\n",
    "make sure you have a worker active.\n",
    "\n",
    "What is the name of the second step in the Automation creation process?\n",
    "\n",
    "- Details\n",
    "- Trigger\n",
    "- Actions\n",
    "- The end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5552947",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "- The name of the second step in the Automation creation process in Prefect Cloud is \"Configure Triggers.\" This step allows you to define the conditions or events that will trigger the Automation to execute. In this case, you would configure the trigger to activate when a flow run completes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de33be80",
   "metadata": {},
   "source": [
    "## Submit the results\n",
    "\n",
    "* Submit your results here: https://forms.gle/nVSYH5fGGamdY1LaA\n",
    "* You can submit your solution multiple times. In this case, only the last submission will be used\n",
    "* If your answer doesn't match options exactly, select the closest one\n",
    "\n",
    "\n",
    "## Deadline\n",
    "\n",
    "The deadline for submitting is 12 June (Monday), 23:00 CEST (Berlin time). \n",
    "\n",
    "After that, the form will be closed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf34682f",
   "metadata": {},
   "source": [
    "# End of The Project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
